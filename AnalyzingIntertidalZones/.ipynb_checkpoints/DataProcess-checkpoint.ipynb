{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf734d5",
   "metadata": {},
   "source": [
    "# Data Processing - Satellite and In Situ\n",
    "\n",
    "In order to analyze and compare in situ field data with ECOSTRESS and EMIT satellite data, you have to prepare your data. These steps will take you from post-field collection to having ready-to-analyze field data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7e441",
   "metadata": {},
   "source": [
    "## Part 1 Convert .sed files to .csv\n",
    "\n",
    "Visualize your data as a csv by converting your .sed files. These files may be in a different form depending on the type of instrument that you will be using. A Spectral Evolution instrument was used for the making of this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c4249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def read_sed_file(filepath, use_column=2):\n",
    "    metadata = {}\n",
    "    wavelengths = []\n",
    "    radiances = []\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Extract metadata\n",
    "    for line in lines:\n",
    "        if line.startswith('Latitude:'):\n",
    "            val = line.split(':')[1].strip()\n",
    "            metadata['latitude'] = float(val) if val.lower() != 'n/a' else None\n",
    "        elif line.startswith('Longitude:'):\n",
    "            val = line.split(':')[1].strip()\n",
    "            metadata['longitude'] = float(val) if val.lower() != 'n/a' else None\n",
    "        elif line.startswith('GPS Time:'):\n",
    "            metadata['gps_time'] = line.split(':', 1)[1].strip()\n",
    "        elif line.strip().startswith('Data:'):\n",
    "            data_start_idx = lines.index(line) + 2\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"No 'Data:' section found in {filepath}\")\n",
    "\n",
    "    for line in lines[data_start_idx:]:\n",
    "        if line.strip():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > use_column:\n",
    "                try:\n",
    "                    wavelengths.append(float(parts[0]))\n",
    "                    radiances.append(float(parts[use_column]))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "    return metadata, wavelengths, radiances\n",
    "\n",
    "def process_sed_directory(directory, output_dir=None, use_column=2):\n",
    "    sed_files = [f for f in os.listdir(directory) if f.lower().endswith('.sed')]\n",
    "    if not sed_files:\n",
    "        raise ValueError(f\"No .sed files found in {directory}.\")\n",
    "\n",
    "    spectra_dict = {}\n",
    "    wavelengths_master = None\n",
    "    meta_records = []\n",
    "\n",
    "    for filename in sed_files:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        metadata, wavelengths, radiances = read_sed_file(filepath, use_column=use_column)\n",
    "        basename = os.path.splitext(filename)[0]\n",
    "\n",
    "        if wavelengths_master is None:\n",
    "            wavelengths_master = wavelengths\n",
    "        elif wavelengths != wavelengths_master:\n",
    "            raise ValueError(f\"Wavelength mismatch in file: {filename}\")\n",
    "\n",
    "        spectra_dict[basename] = radiances\n",
    "\n",
    "        meta_records.append({\n",
    "            'filename': basename,\n",
    "            'latitude': metadata.get('latitude'),\n",
    "            'longitude': metadata.get('longitude'),\n",
    "            'gps_time': metadata.get('gps_time')\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(spectra_dict, index=wavelengths_master)\n",
    "    df.index.name = 'wavelength'\n",
    "    meta_df = pd.DataFrame(meta_records)\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df.to_csv(os.path.join(output_dir, 'spectra_data.csv'))\n",
    "        meta_df.to_csv(os.path.join(output_dir, 'spectra_metadata.csv'), index=False)\n",
    "\n",
    "    return df, meta_df\n",
    "\n",
    "def extract_features(spectra_df):\n",
    "    features = []\n",
    "    for col in spectra_df.columns:\n",
    "        spectrum = spectra_df[col]\n",
    "        features.append([\n",
    "            np.mean(spectrum),\n",
    "            np.max(spectrum),\n",
    "            spectrum.idxmax(),\n",
    "            spectrum.iloc[-1] / spectrum.iloc[0] if spectrum.iloc[0] != 0 else 0,\n",
    "            np.std(spectrum) / np.mean(spectrum) if np.mean(spectrum) != 0 else 0\n",
    "        ])\n",
    "    feature_names = ['mean', 'max', 'peak_wavelength', 'slope', 'flatness']\n",
    "    return pd.DataFrame(features, index=spectra_df.columns, columns=feature_names)\n",
    "\n",
    "def cluster_spectra(features_df, n_clusters=3):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "    labels = kmeans.fit_predict(features_df)\n",
    "    features_df['cluster'] = labels\n",
    "    return features_df\n",
    "\n",
    "def plot_clusters(spectra_df, cluster_labels, output_dir=None):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_clusters))\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    for idx, col in enumerate(spectra_df.columns):\n",
    "        cluster_id = cluster_labels[idx]\n",
    "        ax.plot(spectra_df.index, spectra_df[col], color=colors(cluster_id), alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel('Wavelength (nm)')\n",
    "    ax.set_ylabel('Radiance (W/mÂ²/sr/nm)')\n",
    "    ax.set_title('Spectra colored by cluster')\n",
    "    plt.grid(True)\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'clustered_spectra.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_cluster_means(spectra_df, cluster_labels, output_dir=None):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_clusters))\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        spectra_in_cluster = spectra_df.iloc[:, np.where(cluster_labels == cluster_id)[0]]\n",
    "        mean_spectrum = spectra_in_cluster.mean(axis=1)\n",
    "        ax.plot(spectra_df.index, mean_spectrum, label=f'Cluster {cluster_id}', color=colors(cluster_id))\n",
    "\n",
    "    ax.set_xlabel('Wavelength (nm)')\n",
    "    ax.set_ylabel('Mean Radiance (W/mÂ²/sr/nm)')\n",
    "    ax.set_title('Cluster Mean Spectra')\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'cluster_means.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "############################################\n",
    "# Process Sed:\n",
    "directory = \".../sedData\" #change to your directory\n",
    "output_dir = '.../sedDataOutputs' #change to your directory\n",
    "spectra_df, metadata_df = process_sed_directory(directory, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1c9207",
   "metadata": {},
   "source": [
    "## Part 2: Normalize Your Data\n",
    "\n",
    "Normalize the data you collected through the code below, this code assumes that the first 6 columns of your csv will not be normalized due to the first column representing \"wavelength\" while columns 1-5 will be the reference used to calibrate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code assumes that the first 6 columns of your csv will not be normalized due to the first column representing \n",
    "\"wavelength\" while columns 1-5 will be the reference used to calibrate the data. \n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load your data\n",
    "df = pd.read_csv(\".../Input.csv\")  # Replace with processed csv filename\n",
    "\n",
    "# Step 2: Filter to keep only rows with wavelength between 400â€“900 nm\n",
    "df_filtered = df[df[\"wavelength\"].between(400, 900)]\n",
    "\n",
    "# Step 3: Preserve the following:\n",
    "# - Wavelength column (index 0)\n",
    "# - Columns B to F (index 1 to 5)\n",
    "preserved = df_filtered.iloc[:, 0:6]\n",
    "\n",
    "# Step 4: Compute row-wise average from columns B to F (index 1 to 5)\n",
    "row_avg = df_filtered.iloc[:, 1:6].mean(axis=1)\n",
    "\n",
    "# Step 5: Normalize columns from index 6 onward (column G and beyond)\n",
    "cols_to_normalize = df_filtered.columns[6:]\n",
    "normalized = df_filtered[cols_to_normalize].div(row_avg, axis=0)\n",
    "\n",
    "# Step 6: Sort normalized columns numerically\n",
    "sorted_cols = sorted(normalized.columns, key=lambda x: float(x.split('_')[-1]))\n",
    "normalized = normalized[sorted_cols]\n",
    "\n",
    "# Step 7: Combine everything together\n",
    "final_df = pd.concat([preserved, normalized], axis=1)\n",
    "\n",
    "# Step 8: Export to CSV (optional)\n",
    "final_df.to_csv(\".../Output.csv\", index=False) #change filepath to export normalized data.\n",
    "\n",
    "# Preview output\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd25e93",
   "metadata": {},
   "source": [
    "## Part 3: Pull Coordinates from Photos\n",
    "\n",
    "If needed, you can pull quadrat coordinates. from the photos taken in the field to have accurate coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88956026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import exifread\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_decimal_from_dms(dms, ref):\n",
    "    degrees = dms.values[0].num / dms.values[0].den\n",
    "    minutes = dms.values[1].num / dms.values[1].den\n",
    "    seconds = dms.values[2].num / dms.values[2].den\n",
    "    decimal = degrees + (minutes / 60.0) + (seconds / 3600.0)\n",
    "    if ref in ['S', 'W']:\n",
    "        decimal = -decimal\n",
    "    return decimal\n",
    "\n",
    "def extract_gps(image_path):\n",
    "    with open(image_path, 'rb') as f:\n",
    "        tags = exifread.process_file(f, details=False)\n",
    "        if 'GPS GPSLatitude' in tags and 'GPS GPSLongitude' in tags:\n",
    "            lat = get_decimal_from_dms(tags['GPS GPSLatitude'], tags['GPS GPSLatitudeRef'].printable)\n",
    "            lon = get_decimal_from_dms(tags['GPS GPSLongitude'], tags['GPS GPSLongitudeRef'].printable)\n",
    "            return lat, lon\n",
    "    return None, None\n",
    "\n",
    "def process_images(folder_path):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.heic')):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            lat, lon = extract_gps(filepath)\n",
    "            if lat is not None and lon is not None:\n",
    "                data.append({'filename': filename, 'latitude': lat, 'longitude': lon})\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('photo_locations.csv', index=False) #change folder name/path to your preference.\n",
    "    print(f\"Extracted {len(df)} locations and saved to photo_locations.csv\")\n",
    "\n",
    "# Example usage\n",
    "process_images('.../quadrat_images_') #folder with images for site.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc0d96",
   "metadata": {},
   "source": [
    "## Part 4: Finding and Downloading ECOSTRESS and EMIT Data\n",
    "\n",
    "Follow the code below to download concurent ECOSTRESS and EMIT Data. You can choose to have the concurrent granules automatically download to a file or manually download it throught the .txt file.\n",
    "\n",
    "The code below is based on VITALS Tutorial 1, but was revised to search for three regions of interest rather than just one. \n",
    "\n",
    "Based on the type and location of the data you want to collect, you can choose whether to get the \"Tiled\" or \"Swath\" land surface temperature and emissivity. \n",
    "\n",
    "The data that will be downloaded can be altered in the \"Filter by file types\" section of each code block. \n",
    "\n",
    "Tiled data is best suited for large, regularly gridded datasets where you want to efficently manage and process individual sections of the overall data. Whereas swath data is used to represent data collected along a specific path or track, often with varying resolution and coverage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03416f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to extract Tiled Data\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import earthaccess\n",
    "\n",
    "def process_site(site_name, polygon_path, out_dir, start_date=\"2025-05-01\", end_date=\"2025-06-30\"):\n",
    "    \"\"\"\n",
    "    For a given site:\n",
    "    - Loads the polygon\n",
    "    - Searches ECOSTRESS and EMIT granules\n",
    "    - Filters to relevant file types\n",
    "    - Saves result URLs in a .txt and .csv file in the site-specific folder\n",
    "    \"\"\"\n",
    "    # Load polygon\n",
    "    poly = gpd.read_file(polygon_path)\n",
    "    minx, miny, maxx, maxy = poly.total_bounds\n",
    "\n",
    "    # Set up temporal range and concept IDs\n",
    "    date_range = (start_date, end_date)\n",
    "    concept_ids = [\n",
    "        \"C2408750690-LPCLOUD\",  # EMIT L2A Reflectance Tiled\n",
    "        \"C2076090826-LPCLOUD\"   # ECOSTRESS L2T LST/Emissivity Tiled\n",
    "    ]\n",
    "\n",
    "    # Search for granules in bounding box\n",
    "    print(f\" Date range searched: {start_date} to {end_date}\")\n",
    "\n",
    "    results = earthaccess.search_data(\n",
    "        concept_id=concept_ids,\n",
    "        bounding_box=(minx, miny, maxx, maxy),\n",
    "        temporal=date_range,\n",
    "        cloud_hosted=True\n",
    "    )\n",
    "\n",
    "    # Split ECOSTRESS and EMIT\n",
    "    emit_granules = [g for g in results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"] == \"EMITL2ARFL\"]\n",
    "    eco_granules  = [g for g in results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"] == \"ECO_L2T_LSTE\"]\n",
    "\n",
    "    # Filter by file types\n",
    "    desired_ecostress_assets = ['_LST.tif', '_QC.tif', '_cloud.tif']\n",
    "    desired_emit_extensions = ['.nc']\n",
    "\n",
    "    filtered_urls = []\n",
    "\n",
    "    for g in eco_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(tag) for tag in desired_ecostress_assets):\n",
    "                filtered_urls.append(url)\n",
    "\n",
    "    for g in emit_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(ext) for ext in desired_emit_extensions):\n",
    "                filtered_urls.append(url)\n",
    "\n",
    "    # Make output folder if needed\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save to .txt\n",
    "    txt_path = os.path.join(out_dir, f\"{site_name}_granules.txt\")\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for url in filtered_urls:\n",
    "            f.write(url + \"\\n\")\n",
    "\n",
    "    print(f\"ðŸ” {site_name} search:\")\n",
    "    print(f\"   ECOSTRESS granules found: {len(eco_granules)}\")\n",
    "    print(f\"   EMIT granules found:     {len(emit_granules)}\")\n",
    "    print(f\"   Total granules found:     {len(emit_granules + eco_granules)}\")\n",
    "    \n",
    "    \n",
    "    # # Save to .csv - Uncomment to download data frame.\n",
    "    # df = pd.DataFrame({'url': filtered_urls})\n",
    "    # df.to_csv(os.path.join(out_dir, f\"{site_name}_granules.csv\"), index=False)\n",
    "    # print(f\"{site_name}: Found {len(filtered_urls)} granule files. Saved to {out_dir}\")\n",
    "\n",
    "    return eco_granules, emit_granules\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for Swath Data\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import earthaccess\n",
    "\n",
    "def process_site(site_name, polygon_path, out_dir, start_date=\"2025-04-01\", end_date=\"2025-05-31\"):\n",
    "    \"\"\"\n",
    "    For a given site:\n",
    "    - Loads the polygon\n",
    "    - Searches ECOSTRESS and EMIT granules\n",
    "    - Filters to relevant file types\n",
    "    - Saves result URLs in a .txt and .csv file in the site-specific folder\n",
    "    \"\"\"\n",
    "    # Load polygon\n",
    "    poly = gpd.read_file(polygon_path)\n",
    "    minx, miny, maxx, maxy = poly.total_bounds\n",
    "\n",
    "    # Set up temporal range and concept IDs \n",
    "    date_range = (start_date, end_date)\n",
    "    concept_ids = [\n",
    "        \"C2408750690-LPCLOUD\",   # EMIT L2A Reflectance (swath)\n",
    "        \"C2076114664-LPCLOUD\"    # ECOSTRESS L2 Swath LSTE\n",
    "    ]\n",
    "\n",
    "    # Search for granules in bounding box\n",
    "    print(f\" Date range searched: {start_date} to {end_date}\")\n",
    "\n",
    "    swath_results = earthaccess.search_data(\n",
    "        concept_id=concept_ids,\n",
    "        bounding_box=(minx, miny, maxx, maxy),\n",
    "        temporal=(start_date, end_date),\n",
    "        cloud_hosted=True\n",
    "    )\n",
    "\n",
    "    print(f\"Total SWATH granules found: {len(swath_results)}\")\n",
    "\n",
    "\n",
    "    # Split ECOSTRESS and EMIT\n",
    "    emit_granules = [g for g in swath_results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"].startswith(\"EMIT\")]\n",
    "    eco_granules  = [g for g in swath_results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"].startswith(\"ECO_L2\")]\n",
    "\n",
    "\n",
    "    # Filter by file types\n",
    "    desired_swath_ecostress_extensions = ['.h5', '.nc', '.hdf5']\n",
    "    desired_swath_emit_extensions = ['.nc']\n",
    "\n",
    "    filtered_urls = []\n",
    "\n",
    "    for g in eco_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(tag) for tag in desired_swath_ecostress_extensions):\n",
    "                filtered_urls.append(url)\n",
    "\n",
    "    for g in emit_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(ext) for ext in desired_swath_emit_extensions):\n",
    "                filtered_urls.append(url)\n",
    "\n",
    "    # Make output folder if needed\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save to .txt\n",
    "    txt_path = os.path.join(out_dir, f\"{site_name}_granules.txt\")\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for url in filtered_urls:\n",
    "            f.write(url + \"\\n\")\n",
    "\n",
    "    print(f\"ðŸ” {site_name} search:\")\n",
    "    print(f\"   ECOSTRESS granules found: {len(eco_granules)}\")\n",
    "    print(f\"   EMIT granules found:     {len(emit_granules)}\")\n",
    "    print(f\"   Total granules found:     {len(emit_granules + eco_granules)}\")\n",
    "\n",
    "    # Save to .csv\n",
    "    df = pd.DataFrame({'url': filtered_urls})\n",
    "    df.to_csv(os.path.join(out_dir, f\"{site_name}swath_granules.csv\"), index=False)\n",
    "\n",
    "    print(f\"{site_name}: Found {len(filtered_urls)} granule files. Saved to {out_dir}\")\n",
    "\n",
    "    return eco_granules, emit_granules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63cf4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Update these with your actual file paths - This can be used for either functio above. Remember the last function you run will be the type of data searched. \n",
    "process_site(\n",
    "    site_name=\"LCDM\",\n",
    "    polygon_path=\"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/GIS/Waypoints/LCDMQuadrat.geojson\",\n",
    "    out_dir=\"/Users/kylamonique/Desktop/JPLFiles/data/LCDM\"\n",
    ")\n",
    "\n",
    "process_site(\n",
    "    site_name=\"SD\",\n",
    "    polygon_path=\"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/GIS/Waypoints/SDQuadrat.geojson\",\n",
    "    out_dir=\"/Users/kylamonique/Desktop/JPLFiles/data/SD\"\n",
    ")\n",
    "\n",
    "process_site(\n",
    "    site_name=\"PV\",\n",
    "    polygon_path=\"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/GIS/Waypoints/PVQuadrat.geojson\",\n",
    "    out_dir=\"/Users/kylamonique/Desktop/JPLFiles/data/PV\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce680171",
   "metadata": {},
   "source": [
    "# Optional: Photo Conversion (HEIC to JPG)\n",
    "\n",
    "If you want to analyze the images you have collected to compare them to your in situ spectra, it is ideal to have it process as JPG or JPEG files, but in the case that your images are in HEIC, use the code below to convert images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd27f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pillow_heif\n",
    "\n",
    "# Register HEIF opener\n",
    "pillow_heif.register_heif_opener()\n",
    "\n",
    "# Set paths\n",
    "source_folder = \"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/FieldData/PV0504/Quadrat Images\" #Folder for HEIC inputs\n",
    "destination_folder = \"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/FieldData/PV0504/Quadrat Images/JPG\" #Folder for JPG outputs\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Convert all HEIC files\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.lower().endswith(\".heic\"):\n",
    "        heic_path = os.path.join(source_folder, filename)\n",
    "        jpg_filename = os.path.splitext(filename)[0] + \".jpg\"\n",
    "        jpg_path = os.path.join(destination_folder, jpg_filename)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(heic_path)\n",
    "            image.save(jpg_path, \"JPEG\")\n",
    "            print(f\"Converted: {filename} -> {jpg_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to convert {filename}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
