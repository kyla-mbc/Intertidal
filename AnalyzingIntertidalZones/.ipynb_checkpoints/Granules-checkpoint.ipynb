{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6e17e5",
   "metadata": {},
   "source": [
    "# Search and Export ECOSTRESS and EMIT Granules\n",
    "\n",
    "Based on the type and location of the data you want to collect, you can choose whether to get the \"Tiled\" or \"Swath\" land surface temperature and emissivity. \n",
    "\n",
    "The data that will be downloaded can be altered in the \"Filter by file types\" section of each code block. \n",
    "\n",
    "Tiled data is best suited for large, regularly gridded datasets where you want to efficently manage and process individual sections of the overall data. Whereas swath data is used to represent data collected along a specific path or track, often with varying resolution and coverage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e500436",
   "metadata": {},
   "source": [
    "## Search for Tiled Land Surface Temperature and Emissivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c9698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is to search for export various site granules at one time. Granules will be downloaded to a .txt file \n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import earthaccess\n",
    "\n",
    "def process_site(site_name, polygon_path, out_dir, start_date=\"2024-01-01\", end_date=\"2025-07-23\"):\n",
    "    \"\"\"\n",
    "    For a given site:\n",
    "    - Loads the polygon\n",
    "    - Searches ECOSTRESS and EMIT granules\n",
    "    - Filters to relevant file types\n",
    "    - Saves result URLs in a .txt and .csv file in the site-specific folder\n",
    "    \"\"\"\n",
    "    # Load polygon\n",
    "    poly = gpd.read_file(polygon_path)\n",
    "    minx, miny, maxx, maxy = poly.total_bounds\n",
    "\n",
    "    # Set up temporal range and concept IDs\n",
    "    date_range = (start_date, end_date)\n",
    "    concept_ids = [\n",
    "        \"C2408750690-LPCLOUD\",  # EMIT L2A Reflectance Tiled\n",
    "        \"C2076090826-LPCLOUD\"   # ECOSTRESS L2T LST/Emissivity Tiled\n",
    "    ]\n",
    "\n",
    "    # Search for granules in bounding box\n",
    "    print(f\" Date range searched: {start_date} to {end_date}\")\n",
    "\n",
    "    results = earthaccess.search_data(\n",
    "        concept_id=concept_ids,\n",
    "        bounding_box=(minx, miny, maxx, maxy),\n",
    "        temporal=date_range,\n",
    "        cloud_hosted=True\n",
    "    )\n",
    "\n",
    "    # Split ECOSTRESS and EMIT\n",
    "    emit_granules = [g for g in results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"] == \"EMITL2ARFL\"]\n",
    "    eco_granules  = [g for g in results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"] == \"ECO_L2T_LSTE\"]\n",
    "\n",
    "    # Filter by file types\n",
    "    desired_ecostress_assets = ['_LST.tif', '_QC.tif', '_cloud.tif']\n",
    "    desired_emit_extensions = ['.nc']\n",
    "\n",
    "    filtered_urls = []\n",
    "\n",
    "    for g in eco_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(tag) for tag in desired_ecostress_assets):\n",
    "                filtered_urls.append(url)\n",
    "\n",
    "    for g in emit_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(ext) for ext in desired_emit_extensions):\n",
    "                filtered_urls.append(url)\n",
    "\n",
    "    # Make output folder if needed\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save to .txt\n",
    "    txt_path = os.path.join(out_dir, f\"{site_name}_granules.txt\")\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for url in filtered_urls:\n",
    "            f.write(url + \"\\n\")\n",
    "\n",
    "    print(f\"   {site_name} search:\")\n",
    "    print(f\"   ECOSTRESS granules found: {len(eco_granules)}\")\n",
    "    print(f\"   EMIT granules found:     {len(emit_granules)}\")\n",
    "    print(f\"   Total granules found:     {len(emit_granules + eco_granules)}\")\n",
    "\n",
    "    # Save to .csv\n",
    "    df = pd.DataFrame({'url': filtered_urls})\n",
    "    df.to_csv(os.path.join(out_dir, f\"{site_name}_granules.csv\"), index=False)\n",
    "\n",
    "    print(f\"{site_name}: Found {len(filtered_urls)} granule files. Saved to {out_dir}\")\n",
    "\n",
    "    return eco_granules, emit_granules\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc46dd",
   "metadata": {},
   "source": [
    "## Search for Swath Land Surface Temperature and Emissivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba059d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is to search for export various site granules at one time. Granules will be downloaded to a .txt file \n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import earthaccess\n",
    "\n",
    "def process_site(site_name, polygon_path, out_dir, start_date=\"2025-04-01\", end_date=\"2025-06-30\"):\n",
    "    \"\"\"\n",
    "    For a given site:\n",
    "    - Loads the polygon\n",
    "    - Searches ECOSTRESS and EMIT granules\n",
    "    - Filters to relevant file types\n",
    "    - Saves result URLs in a .txt and .csv file in the site-specific folder\n",
    "    \"\"\"\n",
    "    # Load polygon\n",
    "    poly = gpd.read_file(polygon_path)\n",
    "    minx, miny, maxx, maxy = poly.total_bounds\n",
    "\n",
    "    # Set up temporal range and concept IDs \n",
    "    date_range = (start_date, end_date)\n",
    "    concept_ids = [\n",
    "        \"C2408750690-LPCLOUD\",   # EMIT L2A Reflectance (swath)\n",
    "        \"C2076114664-LPCLOUD\"    # ECOSTRESS L2 Swath LSTE\n",
    "    ]\n",
    "\n",
    "    # Search for granules in bounding box\n",
    "    print(f\" Date range searched: {start_date} to {end_date}\")\n",
    "\n",
    "    swath_results = earthaccess.search_data(\n",
    "        concept_id=concept_ids,\n",
    "        bounding_box=(minx, miny, maxx, maxy),\n",
    "        temporal=(start_date, end_date),\n",
    "        cloud_hosted=True\n",
    "    )\n",
    "\n",
    "    print(f\"Total SWATH granules found: {len(swath_results)}\")\n",
    "\n",
    "\n",
    "    # Split ECOSTRESS and EMIT\n",
    "    emit_granules = [g for g in swath_results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"].startswith(\"EMIT\")]\n",
    "    eco_granules  = [g for g in swath_results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"].startswith(\"ECO_L2\")]\n",
    "\n",
    "\n",
    "    # Filter by file types\n",
    "    desired_swath_ecostress_extensions = ['.h5', '.nc', '.hdf5']\n",
    "    desired_swath_emit_extensions = ['.nc']\n",
    "\n",
    "    filtered_urls = []\n",
    "\n",
    "    for g in eco_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(tag) for tag in desired_swath_ecostress_extensions):\n",
    "                filtered_urls.append(url)\n",
    "\n",
    "    for g in emit_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(ext) for ext in desired_swath_emit_extensions):\n",
    "                filtered_urls.append(url)\n",
    "\n",
    "    # Make output folder if needed\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save to .txt\n",
    "    txt_path = os.path.join(out_dir, f\"{site_name}_granules.csv\")\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for url in filtered_urls:\n",
    "            f.write(url + \"\\n\")\n",
    "\n",
    "    print(f\"üîç {site_name} search:\")\n",
    "    print(f\"   ECOSTRESS granules found: {len(eco_granules)}\")\n",
    "    print(f\"   EMIT granules found:     {len(emit_granules)}\")\n",
    "    print(f\"   Total granules found:     {len(emit_granules + eco_granules)}\")\n",
    "\n",
    "    # # Save to .csv\n",
    "    # df = pd.DataFrame({'url': filtered_urls})\n",
    "    # df.to_csv(os.path.join(out_dir, f\"{site_name}swath_granules.csv\"), index=False)\n",
    "\n",
    "    # print(f\"{site_name}: Found {len(filtered_urls)} granule files. Saved to {out_dir}\")\n",
    "\n",
    "    return eco_granules, emit_granules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67acfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these with your actual file paths\n",
    "process_site(\n",
    "    site_name=\"LCDM\",\n",
    "    polygon_path=\"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/GIS/Waypoints/LCDMQuadrat.geojson\",\n",
    "    out_dir=\"/Users/kylamonique/Desktop/JPLFiles/data/LCDM\"\n",
    ")\n",
    "\n",
    "process_site(\n",
    "    site_name=\"SD\",\n",
    "    polygon_path=\"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/GIS/Waypoints/SDQuadrat.geojson\",\n",
    "    out_dir=\"/Users/kylamonique/Desktop/JPLFiles/data/SD\"\n",
    ")\n",
    "\n",
    "process_site(\n",
    "    site_name=\"PV\",\n",
    "    polygon_path=\"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/GIS/Waypoints/PVQuadrat.geojson\",\n",
    "    out_dir=\"/Users/kylamonique/Desktop/JPLFiles/data/PV\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import earthaccess\n",
    "\n",
    "# Load ROI\n",
    "poi = gpd.read_file(\"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/GIS/Waypoints/LCDMQuadrat.geojson\")\n",
    "minx, miny, maxx, maxy = poi.total_bounds\n",
    "\n",
    "# Date range\n",
    "date_range = (\"2025-04-01\", \"2025-05-31\")\n",
    "\n",
    "# Swath concept IDs\n",
    "concept_ids = [\n",
    "    \"C2408750690-LPCLOUD\",   # EMIT L2A Reflectance (swath)\n",
    "    \"C2076114664-LPCLOUD\"    # ECOSTRESS L2 Swath LSTE :contentReference[oaicite:1]{index=1}\n",
    "]\n",
    "\n",
    "# Search swath granules\n",
    "swath_results = earthaccess.search_data(\n",
    "    concept_id=concept_ids,\n",
    "    bounding_box=(minx, miny, maxx, maxy),\n",
    "    temporal=date_range,\n",
    "    cloud_hosted=True\n",
    ")\n",
    "\n",
    "# Separate swath products\n",
    "emit_swath = [g for g in swath_results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"].startswith(\"EMIT\")]\n",
    "eco_swath  = [g for g in swath_results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"].startswith(\"ECO_L2_LSTE\")]\n",
    "\n",
    "print(f\"Swath granules total: {len(swath_results)}\")\n",
    "print(f\"EMIT swath granules: {len(emit_swath)}\")\n",
    "print(f\"ECOSTRESS swath granules: {len(eco_swath)}\")\n",
    "\n",
    "# Write swath asset URLs to file\n",
    "output = \"swath_granules.txt\"\n",
    "with open(output, \"w\") as f:\n",
    "    for g in swath_results:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if url.endswith(('.nc', '.h5', '.hdf5')):\n",
    "                f.write(url + \"\\n\")\n",
    "\n",
    "print(f\"Swath granule links saved to {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c05b34",
   "metadata": {},
   "source": [
    "Manually input lat and lon locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import earthaccess\n",
    "\n",
    "def process_site_from_bbox(site_name, bounding_box, out_dir, start_date=\"2024-01-01\", end_date=\"2025-06-30\", save_results=True):\n",
    "    \"\"\"\n",
    "    For a given site:\n",
    "    - Searches ECOSTRESS and EMIT granules using manually input bounding box\n",
    "    - Filters to relevant file types\n",
    "    - Optionally saves result URLs in a .txt file\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack bounding box\n",
    "    min_lon, min_lat, max_lon, max_lat = bounding_box\n",
    "    print(f\"\\n Searching site: '{site_name}'\")\n",
    "    print(f\" Bounding Box: Lon [{min_lon}, {max_lon}], Lat [{min_lat}, {max_lat}]\")\n",
    "    print(f\" Date Range: {start_date} to {end_date}\")\n",
    "\n",
    "    # Earthdata concept IDs\n",
    "    concept_ids = [\n",
    "        \"C2408750690-LPCLOUD\",  # EMIT L2A Reflectance (swath)\n",
    "        \"C2076114664-LPCLOUD\"   # ECOSTRESS L2 Swath LSTE\n",
    "    ]\n",
    "\n",
    "    # Perform search\n",
    "    swath_results = earthaccess.search_data(\n",
    "        concept_id=concept_ids,\n",
    "        bounding_box=(min_lon, min_lat, max_lon, max_lat),\n",
    "        temporal=(start_date, end_date),\n",
    "        cloud_hosted=True\n",
    "    )\n",
    "\n",
    "    print(f\" Total granules found: {len(swath_results)}\")\n",
    "\n",
    "    # Filter granules by mission\n",
    "    emit_granules = [g for g in swath_results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"].startswith(\"EMIT\")]\n",
    "    eco_granules  = [g for g in swath_results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"].startswith(\"ECO_L2\")]\n",
    "\n",
    "    # Define desired file extensions\n",
    "    desired_ecostress_ext = ['.h5', '.nc', '.hdf5']\n",
    "    desired_emit_ext = ['.nc']\n",
    "\n",
    "    # Extract URLs\n",
    "    filtered_urls = []\n",
    "    for g in eco_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(ext) for ext in desired_ecostress_ext):\n",
    "                filtered_urls.append(url)\n",
    "    for g in emit_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(ext) for ext in desired_emit_ext):\n",
    "                filtered_urls.append(url)\n",
    "\n",
    "    # Save results to file\n",
    "    if save_results:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        txt_path = os.path.join(out_dir, f\"{site_name}_granules.txt\")\n",
    "        with open(txt_path, \"w\") as f:\n",
    "            for url in filtered_urls:\n",
    "                f.write(url + \"\\n\")\n",
    "        print(f\"üìÅ Saved granule URLs to: {txt_path}\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n {site_name} Summary:\")\n",
    "    print(f\"   ECOSTRESS granules: {len(eco_granules)}\")\n",
    "    print(f\"   EMIT granules:     {len(emit_granules)}\")\n",
    "    print(f\"   Filtered URLs:     {len(filtered_urls)}\")\n",
    "\n",
    "    return eco_granules, emit_granules\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
