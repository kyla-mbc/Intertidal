{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf734d5",
   "metadata": {},
   "source": [
    "# Data Processing - Satellite and In Situ\n",
    "\n",
    "In order to analyze and compare in situ field data with ECOSTRESS and EMIT satellite data, you have to prepare your data. These steps will take you from post-field collection to having ready-to-analyze field data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7e441",
   "metadata": {},
   "source": [
    "## Part 1 Convert .sed files to .csv\n",
    "\n",
    "Visualize your data as a csv by converting your .sed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c4249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def read_sed_file(filepath, use_column=2):\n",
    "    metadata = {}\n",
    "    wavelengths = []\n",
    "    radiances = []\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Extract metadata\n",
    "    for line in lines:\n",
    "        if line.startswith('Latitude:'):\n",
    "            val = line.split(':')[1].strip()\n",
    "            metadata['latitude'] = float(val) if val.lower() != 'n/a' else None\n",
    "        elif line.startswith('Longitude:'):\n",
    "            val = line.split(':')[1].strip()\n",
    "            metadata['longitude'] = float(val) if val.lower() != 'n/a' else None\n",
    "        elif line.startswith('GPS Time:'):\n",
    "            metadata['gps_time'] = line.split(':', 1)[1].strip()\n",
    "        elif line.strip().startswith('Data:'):\n",
    "            data_start_idx = lines.index(line) + 2\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"No 'Data:' section found in {filepath}\")\n",
    "\n",
    "    for line in lines[data_start_idx:]:\n",
    "        if line.strip():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > use_column:\n",
    "                try:\n",
    "                    wavelengths.append(float(parts[0]))\n",
    "                    radiances.append(float(parts[use_column]))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "    return metadata, wavelengths, radiances\n",
    "\n",
    "def process_sed_directory(directory, output_dir=None, use_column=2):\n",
    "    sed_files = [f for f in os.listdir(directory) if f.lower().endswith('.sed')]\n",
    "    if not sed_files:\n",
    "        raise ValueError(f\"No .sed files found in {directory}.\")\n",
    "\n",
    "    spectra_dict = {}\n",
    "    wavelengths_master = None\n",
    "    meta_records = []\n",
    "\n",
    "    for filename in sed_files:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        metadata, wavelengths, radiances = read_sed_file(filepath, use_column=use_column)\n",
    "        basename = os.path.splitext(filename)[0]\n",
    "\n",
    "        if wavelengths_master is None:\n",
    "            wavelengths_master = wavelengths\n",
    "        elif wavelengths != wavelengths_master:\n",
    "            raise ValueError(f\"Wavelength mismatch in file: {filename}\")\n",
    "\n",
    "        spectra_dict[basename] = radiances\n",
    "\n",
    "        meta_records.append({\n",
    "            'filename': basename,\n",
    "            'latitude': metadata.get('latitude'),\n",
    "            'longitude': metadata.get('longitude'),\n",
    "            'gps_time': metadata.get('gps_time')\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(spectra_dict, index=wavelengths_master)\n",
    "    df.index.name = 'wavelength'\n",
    "    meta_df = pd.DataFrame(meta_records)\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df.to_csv(os.path.join(output_dir, 'spectra_data.csv'))\n",
    "        meta_df.to_csv(os.path.join(output_dir, 'spectra_metadata.csv'), index=False)\n",
    "\n",
    "    return df, meta_df\n",
    "\n",
    "def extract_features(spectra_df):\n",
    "    features = []\n",
    "    for col in spectra_df.columns:\n",
    "        spectrum = spectra_df[col]\n",
    "        features.append([\n",
    "            np.mean(spectrum),\n",
    "            np.max(spectrum),\n",
    "            spectrum.idxmax(),\n",
    "            spectrum.iloc[-1] / spectrum.iloc[0] if spectrum.iloc[0] != 0 else 0,\n",
    "            np.std(spectrum) / np.mean(spectrum) if np.mean(spectrum) != 0 else 0\n",
    "        ])\n",
    "    feature_names = ['mean', 'max', 'peak_wavelength', 'slope', 'flatness']\n",
    "    return pd.DataFrame(features, index=spectra_df.columns, columns=feature_names)\n",
    "\n",
    "def cluster_spectra(features_df, n_clusters=3):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "    labels = kmeans.fit_predict(features_df)\n",
    "    features_df['cluster'] = labels\n",
    "    return features_df\n",
    "\n",
    "def plot_clusters(spectra_df, cluster_labels, output_dir=None):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_clusters))\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    for idx, col in enumerate(spectra_df.columns):\n",
    "        cluster_id = cluster_labels[idx]\n",
    "        ax.plot(spectra_df.index, spectra_df[col], color=colors(cluster_id), alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel('Wavelength (nm)')\n",
    "    ax.set_ylabel('Radiance (W/m²/sr/nm)')\n",
    "    ax.set_title('Spectra colored by cluster')\n",
    "    plt.grid(True)\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'clustered_spectra.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_cluster_means(spectra_df, cluster_labels, output_dir=None):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_clusters))\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        spectra_in_cluster = spectra_df.iloc[:, np.where(cluster_labels == cluster_id)[0]]\n",
    "        mean_spectrum = spectra_in_cluster.mean(axis=1)\n",
    "        ax.plot(spectra_df.index, mean_spectrum, label=f'Cluster {cluster_id}', color=colors(cluster_id))\n",
    "\n",
    "    ax.set_xlabel('Wavelength (nm)')\n",
    "    ax.set_ylabel('Mean Radiance (W/m²/sr/nm)')\n",
    "    ax.set_title('Cluster Mean Spectra')\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    if output_dir:\n",
    "        plt.savefig(os.path.join(output_dir, 'cluster_means.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "############################################\n",
    "# Process Sed:\n",
    "directory = \".../sedData\" #change to your directory\n",
    "output_dir = '.../sedDataOutputs' #change to your directory\n",
    "spectra_df, metadata_df = process_sed_directory(directory, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1c9207",
   "metadata": {},
   "source": [
    "## Part 2: Normalize Your Data\n",
    "\n",
    "Normalize the data you collected through the code below, this code assumes that the first 6 columns of your csv will not be normalized due to the first column representing \"wavelength\" while columns 1-5 will be the reference used to calibrate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code assumes that the first 6 columns of your csv will not be normalized due to the first column representing \n",
    "\"wavelength\" while columns 1-5 will be the reference used to calibrate the data. \n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load your data\n",
    "df = pd.read_csv(\".../Input.csv\")  # Replace with processed csv filename\n",
    "\n",
    "# Step 2: Filter to keep only rows with wavelength between 400–900 nm\n",
    "df_filtered = df[df[\"wavelength\"].between(400, 900)]\n",
    "\n",
    "# Step 3: Preserve the following:\n",
    "# - Wavelength column (index 0)\n",
    "# - Columns B to F (index 1 to 5)\n",
    "preserved = df_filtered.iloc[:, 0:6]\n",
    "\n",
    "# Step 4: Compute row-wise average from columns B to F (index 1 to 5)\n",
    "row_avg = df_filtered.iloc[:, 1:6].mean(axis=1)\n",
    "\n",
    "# Step 5: Normalize columns from index 6 onward (column G and beyond)\n",
    "cols_to_normalize = df_filtered.columns[6:]\n",
    "normalized = df_filtered[cols_to_normalize].div(row_avg, axis=0)\n",
    "\n",
    "# Step 6: Sort normalized columns numerically\n",
    "sorted_cols = sorted(normalized.columns, key=lambda x: float(x.split('_')[-1]))\n",
    "normalized = normalized[sorted_cols]\n",
    "\n",
    "# Step 7: Combine everything together\n",
    "final_df = pd.concat([preserved, normalized], axis=1)\n",
    "\n",
    "# Step 8: Export to CSV (optional)\n",
    "final_df.to_csv(\".../Output.csv\", index=False) #change filepath to export normalized data.\n",
    "\n",
    "# Preview output\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd25e93",
   "metadata": {},
   "source": [
    "## Part 3: Pull Coordinates from Photos\n",
    "\n",
    "If needed, you can pull quadrat coordinates. from the photos taken in the field to have accurate coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88956026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import exifread\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_decimal_from_dms(dms, ref):\n",
    "    degrees = dms.values[0].num / dms.values[0].den\n",
    "    minutes = dms.values[1].num / dms.values[1].den\n",
    "    seconds = dms.values[2].num / dms.values[2].den\n",
    "    decimal = degrees + (minutes / 60.0) + (seconds / 3600.0)\n",
    "    if ref in ['S', 'W']:\n",
    "        decimal = -decimal\n",
    "    return decimal\n",
    "\n",
    "def extract_gps(image_path):\n",
    "    with open(image_path, 'rb') as f:\n",
    "        tags = exifread.process_file(f, details=False)\n",
    "        if 'GPS GPSLatitude' in tags and 'GPS GPSLongitude' in tags:\n",
    "            lat = get_decimal_from_dms(tags['GPS GPSLatitude'], tags['GPS GPSLatitudeRef'].printable)\n",
    "            lon = get_decimal_from_dms(tags['GPS GPSLongitude'], tags['GPS GPSLongitudeRef'].printable)\n",
    "            return lat, lon\n",
    "    return None, None\n",
    "\n",
    "def process_images(folder_path):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.heic')):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            lat, lon = extract_gps(filepath)\n",
    "            if lat is not None and lon is not None:\n",
    "                data.append({'filename': filename, 'latitude': lat, 'longitude': lon})\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('photo_locations.csv', index=False) #change folder name/path to your preference.\n",
    "    print(f\"Extracted {len(df)} locations and saved to photo_locations.csv\")\n",
    "\n",
    "# Example usage\n",
    "process_images('.../quadrat_images_') #folder with images for site.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc0d96",
   "metadata": {},
   "source": [
    "## Part 4: Finding and Downloading ECOSTRESS and EMIT Data\n",
    "\n",
    "Follow the code below to download concurent ECOSTRESS and EMIT Data. You can choose to have the concurrent granules automatically download to a file or manually download it throught the .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03416f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import earthaccess\n",
    "\n",
    "def process_site(site_name, polygon_path, out_dir, start_date=\"2025-05-15\", end_date=\"2025-05-31\"):\n",
    "    \"\"\"\n",
    "    For a given site:\n",
    "    - Loads the polygon\n",
    "    - Searches ECOSTRESS and EMIT granules\n",
    "    - Filters to relevant file types\n",
    "    - Saves result URLs in a .txt and .csv file in the site-specific folder\n",
    "    \"\"\"\n",
    "    # Load polygon\n",
    "    poly = gpd.read_file(polygon_path)\n",
    "    minx, miny, maxx, maxy = poly.total_bounds\n",
    "\n",
    "    # Set up temporal range and concept IDs\n",
    "    date_range = (start_date, end_date)\n",
    "    concept_ids = [\n",
    "        \"C2408750690-LPCLOUD\",  # EMIT L2A Reflectance Tiled\n",
    "        \"C2076090826-LPCLOUD\"   # ECOSTRESS L2T LST/Emissivity Tiled\n",
    "    ]\n",
    "\n",
    "    # Search for granules in bounding box\n",
    "    print(f\" Date range searched: {start_date} to {end_date}\")\n",
    "\n",
    "    results = earthaccess.search_data(\n",
    "        concept_id=concept_ids,\n",
    "        bounding_box=(minx, miny, maxx, maxy),\n",
    "        temporal=date_range,\n",
    "        cloud_hosted=True\n",
    "    )\n",
    "\n",
    "    # Split ECOSTRESS and EMIT\n",
    "    emit_granules = [g for g in results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"] == \"EMITL2ARFL\"]\n",
    "    eco_granules  = [g for g in results if g[\"umm\"][\"CollectionReference\"][\"ShortName\"] == \"ECO_L2T_LSTE\"]\n",
    "\n",
    "    # Filter by file types\n",
    "    desired_ecostress_assets = ['_LST.tif', '_QC.tif', '_cloud.tif']\n",
    "    desired_emit_extensions = ['.nc']\n",
    "\n",
    "    filtered_urls = []\n",
    "\n",
    "    for g in eco_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(tag) for tag in desired_ecostress_assets):\n",
    "                filtered_urls.append(url)\n",
    "\n",
    "    for g in emit_granules:\n",
    "        for url in g.data_links(access=\"https\"):\n",
    "            if any(url.endswith(ext) for ext in desired_emit_extensions):\n",
    "                filtered_urls.append(url)\n",
    "\n",
    "    # Make output folder if needed\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save to .txt\n",
    "    txt_path = os.path.join(out_dir, f\"{site_name}_granules.txt\")\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for url in filtered_urls:\n",
    "            f.write(url + \"\\n\")\n",
    "\n",
    "    print(f\"🔍 {site_name} search:\")\n",
    "    print(f\"   ECOSTRESS granules found: {len(eco_granules)}\")\n",
    "    print(f\"   EMIT granules found:     {len(emit_granules)}\")\n",
    "    print(f\"   Total granules found:     {len(emit_granules + eco_granules)}\")\n",
    "\n",
    "    return eco_granules, emit_granules\n",
    "\n",
    "\n",
    "    # # Save to .csv - Uncomment to download data frame.\n",
    "    # df = pd.DataFrame({'url': filtered_urls})\n",
    "    # df.to_csv(os.path.join(out_dir, f\"{site_name}_granules.csv\"), index=False)\n",
    "\n",
    "    # print(f\"✅ {site_name}: Found {len(filtered_urls)} granule files. Saved to {out_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these with your actual file paths\n",
    "process_site(\n",
    "    site_name=\"LCDM\",\n",
    "    polygon_path=\"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/GIS/Waypoints/LCDMQuadrat.geojson\",\n",
    "    out_dir=\"/Users/kylamonique/Desktop/JPLFiles/data/LCDM\"\n",
    ")\n",
    "\n",
    "process_site(\n",
    "    site_name=\"SD\",\n",
    "    polygon_path=\"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/GIS/Waypoints/SDQuadrat.geojson\",\n",
    "    out_dir=\"/Users/kylamonique/Desktop/JPLFiles/data/SD\"\n",
    ")\n",
    "\n",
    "process_site(\n",
    "    site_name=\"PV\",\n",
    "    polygon_path=\"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/GIS/Waypoints/PVQuadrat.geojson\",\n",
    "    out_dir=\"/Users/kylamonique/Desktop/JPLFiles/data/PV\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce680171",
   "metadata": {},
   "source": [
    "# Optional: Photo Conversion (HEIC to JPG)\n",
    "\n",
    "If you want to analyze the images you have collected to compare them to your in situ spectra, it is ideal to have it process as JPG or JPEG files, but in the case that your images are in HEIC, use the code below to convert images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd27f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pillow_heif\n",
    "\n",
    "# Register HEIF opener\n",
    "pillow_heif.register_heif_opener()\n",
    "\n",
    "# Set paths\n",
    "source_folder = \"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/FieldData/PV0504/Quadrat Images\" #Folder for HEIC inputs\n",
    "destination_folder = \"/Users/kylamonique/Desktop/JPLFiles/SpectralEvolution/FieldData/PV0504/Quadrat Images/JPG\" #Folder for JPG outputs\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Convert all HEIC files\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.lower().endswith(\".heic\"):\n",
    "        heic_path = os.path.join(source_folder, filename)\n",
    "        jpg_filename = os.path.splitext(filename)[0] + \".jpg\"\n",
    "        jpg_path = os.path.join(destination_folder, jpg_filename)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(heic_path)\n",
    "            image.save(jpg_path, \"JPEG\")\n",
    "            print(f\"Converted: {filename} -> {jpg_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to convert {filename}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
